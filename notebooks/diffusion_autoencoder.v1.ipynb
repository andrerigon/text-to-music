{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Auto Encoder\n",
    "\n",
    "This is a implementation from [audio-diffusion-pytorch](https://github.com/archinetai/audio-diffusion-pytorch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from audio_diffusion_pytorch import DiffusionAE, UNetV0, VDiffusion, VSampler\n",
    "from audio_encoders_pytorch import MelE1d, TanhBottleneck\n",
    "\n",
    "from src.datasets import MusicCapsDataset\n",
    "from src.features import PreProcessor\n",
    "from src.features.extractor import WaveformExtractor\n",
    "from src.utils.data import TorchDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset\n",
    "\n",
    "Each Musiccaps dataset has 10-second audios. To adapt to this model we will apply the following changes to the data:\n",
    "\n",
    "* We will divide the data into two parts of 5.5 seconds. Each part of the audio will have two times: from 0 to 5.5 and 4.5 to 10 seconds.\n",
    "* For each new audio, we will add information to the 'aspect list' and 'caption' explaining the audio track. For example '1 of 2' or '2 of 2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musiccaps_generator = MusicCapsDataset(format=\"mp3\", crop_length=5.5)\n",
    "dataset = musiccaps_generator.generate(num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "For the format defined by the article, we need to cut the song to size 2**18 (approximately 5.5 seconds), so that it adapts to the network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: We don't need to save waveforms.\n",
    "train, test = PreProcessor(dataset, lambda dataset : WaveformExtractor(dataset, column=\"audio\", crop_length=2**18)).get_train_test_split(path=musiccaps_generator.get_processed_folder(), save_split_sets=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting train data\n",
    "\n",
    "We need to adapt the standard pytorch data:\n",
    "\n",
    "* The pattern suggested in the documentation is [batch, in_channels, length]. In our case, our dataset is in the format [batch, length]. (length is the multiplication of frequency by time).\n",
    "* We need to use DataLoader, an optimized implementation to access our data.\n",
    "* We take the opportunity to use the gpu, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 8\n",
    "transform = lambda x: x.unsqueeze(0) # (batch, length) ->  (batch, 1, length)\n",
    "\n",
    "train_dataset = TorchDataset(train, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "We use the same as the original example. We add an optimizer to update the parameters of the model based on the gradients computed during backpropagation.\n",
    "The loss is calculated internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = DiffusionAE(\n",
    "    encoder=MelE1d(\n",
    "        in_channels=1,\n",
    "        channels=512,\n",
    "        multipliers=[1, 1],\n",
    "        factors=[2],\n",
    "        num_blocks=[12],\n",
    "        out_channels=32,\n",
    "        mel_channels=80,\n",
    "        mel_sample_rate=48000,\n",
    "        mel_normalize_log=True,\n",
    "        bottleneck=TanhBottleneck(),\n",
    "    ),\n",
    "    inject_depth=6,\n",
    "    net_t=UNetV0,\n",
    "    in_channels=1,\n",
    "    channels=[8, 32, 64, 128, 256, 512, 512, 1024, 1024],\n",
    "    factors=[1, 4, 4, 4, 2, 2, 2, 2, 2],\n",
    "    items=[1, 2, 2, 2, 2, 2, 2, 4, 4],\n",
    "    diffusion_t=VDiffusion,\n",
    "    sampler_t=VSampler,\n",
    "    )\n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(10)): # Number of epochs\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = autoencoder(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
